{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqFbRsllaj5tpt1LWzPRIV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Automatic Differentiation\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eleni-vasilaki/rl-notes/blob/main/notebooks/07_deltarule.ipynb)\n","## Evaluating partial derivatives efficiently\n","\n","Gradient-based optimisation is ubiquitous in machine learning. Driven by the backpropagation algorithm, neural networks are able to minimise loss functions by gradually stepping parameters in the direction that reduces the overall error of the network. To calculate the direction we should change our parameters, we first need to consider how the loss changes in response to a change in parameters. This is quantified by the partial derivative of loss, $L$, with respect to a given parameter, $\\theta$: $\\frac{âˆ‚L}{\\partial \\theta}$\n","\n","Since neural networks feature many parameters (on the order of 1.8 trillion parameters for ChatGPT 4.0), calculating these gradients efficiently is crucial for optimising implementations. At the heart of this process is automatic differentiation, an algorithmic approach for calculating the exact partial derivatives of parameters of a function with respect to its outputs.\n","\n","This process requires no symbolic representation of derivatives, nor does it perform numerical estimates. Instead, it propagates information on gradients through a computational graph consisting of intermediate variables (intermediate steps the computational algorithm arrives at in the execution of a function), and elementary mathematical operations (plus, minus, divide, etc.)/ functions (sin, cos, exp) with known derivatives. Compounded operations are sequentially performed via the chain rule, and gradient information with respect to specific inputs to the function is contained numerically: \\\\\n","$\\frac{\\partial f(g(x))}{\\partial x} = f'(g(x))\\cdot g'(x)$\n","\n","In the lecture, we covered how a computational graph can be constructed by breaking down a mathematical function in the same way a computer would process it: single mathematical operations on a (pair of) variable(s). Below, we will outline the process for generating a computational graph.\n","\n","## Building Computational Graphs\n","\n","Consider the function:\\\n","\\\n","$f(x_0,x_1) = (\\frac{x_0^2}{x_1})\\cdot \\mathrm{cos}(x_0) + e^{\\frac{x_0}{x_1}}$ \\\\\n","\\\n","we can simplify this function to a series of operations, represented by intermediate variables, $v_i$, which at each stage perform either an elementary function on one variable, or an elementary operation on a pair of variables.\n","\n","First we will start with mapping each of our inputs to an intermediate variable: \\\\\n","\\\n","$v_0 = x_0$ \\\\\n","$v_1 = x_1$ \\\\\n","\\\n","From here, we can see that the fraction $\\frac{x_0}{x_1}$ appears twice (though once is multiplied again by $x_0$). We can then express these new intermediate variables with respect to the previous intermediate variables: \\\\\n","\\\n","$v_2 = \\frac{v_0}{v_1}$ \\\\\n","$v_3 = v_0 \\cdot v_2$ \\\\\n","\\\n","No repeated terms remain, so we step through the function according to order of mathematical operations (BIDMAS/BODMAS/PEMDAS): \\\\\n","\\\n","$v_4 = e^v_2$ \\\\\n","$v_5 = \\mathrm{cos}(v_0)$ \\\\\n","$v_6 = v_3 \\cdot v_5$ \\\\\n","$v_7 = v_4 + v_6$ \\\\\n","\\\n","We have arrived at the end of the function, with output equal to $v_7$. \\\\\n","\n","# Graphical Expression\n","To convey this information visually, we can draw a graph of how each of these intermediate variables relate to one another. Nodes on the graph will represent intermediate variables, and edges will denote dependence between variables. Arrows pointing into an intermediate variable means that this intermediate variable takes the connected node as an input. Arrows out of an intermediate variable show that another intermediate variable is dependent upon it:\n","\\\n","<img src='https://drive.google.com/uc?id=1SycSLZ8kvsyQgo9_--S_PmvOpH5W-ZNm'>\n","\n","\n","\n","By looking at the intermediate variables we generated above for the given function, we can use this notation to plot a computational graph which represents our function:\n","\\\n","$f(x_0,x_1) = (\\frac{x_0^2}{x_1})\\cdot \\mathrm{cos}(x_0) + e^{\\frac{x_0}{x_1}}$ \\\\\n","<img src='https://drive.google.com/uc?id=1VvOmzRom-v20HDUKyJJUWBlSm8vfdgHQ'>\n","\n","\n","\n","\n"],"metadata":{"id":"MHMqlIvAXQSn"}},{"cell_type":"markdown","source":["# Exercise:\n","Write a python function that represents the equation\n","$y=\\mathrm{sin}(x_0) + x_1\\mathrm{cos}(x_0) + \\frac{x_1}{x_2}$\n","as a series of operations on intermediate variables."],"metadata":{"id":"NJeA84_Mop_J"}},{"cell_type":"code","source":["import numpy as np\n","\n","def function(x0, x1, x2):\n"],"metadata":{"id":"2Z0JMpTQQn-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<details>\n","<summary>Show Solution</summary>\n","\n","```python\n","# Solution:\n","import numpy as np\n","\n","def function(x0, x1, x2):\n","  v0 = x0\n","  v1 = x1\n","  v2 = x2\n","  v3 = np.sin(v0)\n","  v4 = np.cos(v0)\n","  v5 = v1 * v4\n","  v6 = v1/v2\n","  v7 = v3+v5\n","  v8 = v6 + v7\n","  return v8\n","  ```"],"metadata":{"id":"H0J0amNaAr7X"}},{"cell_type":"markdown","source":["# Exercise:\n","\n","Draw a computational graph that represents the function you have made."],"metadata":{"id":"tuQDveSzqoQm"}},{"cell_type":"markdown","source":["<details>\n","<summary>Show Solution</summary>\n","\n","Solution:\n","\n","<img src='https://drive.google.com/uc?id=1XiDJ1Y_t7JO6h-k53wswnR-T5end9tS2'>"],"metadata":{"id":"FxCn3rN7rU5Y"}},{"cell_type":"markdown","source":["# Forward-Mode Automatic Differentiation\n","Forward-mode automatic differentiation refers to an automatic differentiation process that propagates gradient information forwards starting at the inputs through the computational graph. It is performed with respect to a single input variable at a time, and computes both the value of each of the intermediate variables, as well as the partial derivative of the intermediate variables with respect to the chosen input at each node of the graph. \\\\\n","\n","This is often achieved through 'operator overloading', where the algorithmic implementation of the function considers both the values on intermediate variables $v_i$, as well as the partial derivative with respect to input $x_i$, $\\frac{\\partial v_1}{\\partial x_i}$, termed the 'seed'. \\\\\n","\\\n","Operations on the derivatives (seeds) are performed according to the differential rules for multiplication, division, and compounded operations: the product, quotient, and chain rules respectively: \\\\\n","$\\frac{\\partial (uv)}{\\partial x} = u \\frac{\\partial v}{\\partial x}+ v \\frac{\\partial u}{\\partial x}$ \\\\\n","$\\frac{\\partial (u/v)}{\\partial x} = \\frac{v \\frac{\\partial u}{\\partial x}- u \\frac{\\partial v}{\\partial x}}{v^2}$ \\\\\n","$\\frac{\\partial f(g(x))}{\\partial x} = f'(g(x))\\cdot g'(x)$ \\\\\n","\n","Considering the function from the earlier: \\\\\n","\n","$f(x_0,x_1) = (\\frac{x_0^2}{x_1})\\cdot \\mathrm{cos}(x_0) + e^{\\frac{x_0}{x_1}}$\n","\\\n","We can write in programmatic form:"],"metadata":{"id":"f4868Y_iu9au"}},{"cell_type":"code","source":["import numpy as np\n","def function1(x0, x1):\n","  v0 = x0\n","  v1 = x1\n","  v2 = v0/v1\n","  v3 = v0 * v2\n","  v4 = np.exp(v2)\n","  v5 = np.cos(v0)\n","  v6 = v3 * v5\n","  v7 = v4 + v6\n","  y = v7\n","  return y"],"metadata":{"id":"FczDlaVUopR4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can add additional terms below each of the intermediate variables to track the partial derivatives as we pass through the function. \\\\\n","While this looks clumsy as a standard python function, in packages that handle automatic differentiation, such as PyTorch, variables are stored as instantiations of a 'tensor' class, and the gradient information of each variable is an attribute of the class, and hence can be accessed when needed by other functions. The function below will return function output y as well as the the partial derivative of y with respect to x:"],"metadata":{"id":"P0SmRMe7ywhY"}},{"cell_type":"code","source":["import numpy as np\n","def function1_dx0(x0, x1):\n","  v0 = x0\n","  dv0dx0 = 1 # The derivative of a variable with respect to itself is 1\n","  v1 = x1\n","  dv1dx0 = 0 # v1 Does not depend on x0, derivative is 0\n","  v2 = v0/v1\n","  dv2dx0 = (v1*dv0dx0-v0*dv1dx0)/v1**2 # Quotient rule, u = v0, par u = dv0dx0, v = v1, par v = dv1dx0\n","  v3 = v0 * v2\n","  dv3dx0 = v0 * dv2dx0 + v2 * dv0dx0 # Product rule, u = v0, par u = dv0dx0, v = v2, par v = dv2dx0\n","  v4 = np.exp(v2)\n","  dv4dx0 = dv2dx0 * np.exp(v2) # Chain rule, f = exp(), par f = exp(), g = v2, par g  = dv2dx0\n","  v5 = np.cos(v0)\n","  dv5dx0 = dv0dx0 * - np.sin(v0) # Chain rule, f = cos(), par f = -sin(), g = v0, par g = dv0dx0\n","  v6 = v3 * v5\n","  dv6dx0 = v5 * dv3dx0 + v3 * dv5dx0 # Product rule, u = v5, par u = dv4dx0, v = v3, par v = dv3dx0\n","  v7 = v4 + v6\n","  dv7dx0 = dv4dx0 + dv6dx0 # Addition of gradients is linear\n","  y = v7\n","  dydx0 = dv7dx0\n","  return y, dydx0"],"metadata":{"id":"3WaD6Essyyg1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Running a comparison between the time taken for automatic differentiation and running the symbolic solution for the equation, we can see that automatic differentiation offers a speedup. While this increase is relatively modest for the function shown, the process offers even better scaling for more complex functions with more compounded operations."],"metadata":{"id":"5qqZwPm_WZ2e"}},{"cell_type":"code","source":["def symbolic_df1dx(x0, x1):\n","  return ((x0)**2/x1)*np.cos(x0)+np.exp(x0/x1), ((x0**2)*(-np.sin(x0))+np.exp(x0/x1)+2*x0*np.cos(x0))/x1\n","import timeit\n","time_automatic = timeit.timeit(lambda: function1_dx0(1,1), number=100000)\n","time_symbolic = timeit.timeit(lambda: symbolic_df1dx(1,1), number=100000)\n","print(f'Time taken by automatic differentiation: {time_automatic}')\n","print(f'Time taken by symbolic differentiation: {time_symbolic}')\n","\n","print('Automatic: y='+str(function1_dx0(1,1)[0])+', dy/dx0 = '+str(function1_dx0(1,1)[1]))\n","print('Symbolic: y='+str(symbolic_df1dx(1,1)[0])+', dy/dx0 = '+str(symbolic_df1dx(1,1)[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbteRprEU0gT","executionInfo":{"status":"ok","timestamp":1746705601269,"user_tz":-60,"elapsed":1248,"user":{"displayName":"Ian Vidamour","userId":"11824567035710900920"}},"outputId":"5106982e-6d23-4914-9f40-2fb8ae3030a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken by automatic differentiation: 0.588525680001112\n","Time taken by symbolic differentiation: 0.6741398379999737\n","Automatic: y=3.258584134327185, dy/dx0 = 2.9574154553874283\n","Symbolic: y=3.258584134327185, dy/dx0 = 2.9574154553874283\n"]}]},{"cell_type":"markdown","source":["# Exercise\n","Perform a similar function generation for the following function:\n","$f(x_0,x_1) = (\\frac{x_0^2}{x_1})\\cdot \\mathrm{cos}(x_0) + e^{\\frac{x_0}{x_1}}$"],"metadata":{"id":"VNoAnpRmQ9KU"}},{"cell_type":"code","source":["def function2_dx0(x0,x1,x2):\n","  # your code here\n","  return y, dydx0"],"metadata":{"id":"BdxJZPt7Rg3m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<details>\n","<summary>Show Solution</summary>\n","\n","```python\n","# Solution\n","def function2_dx0(x0, x1, x2):\n","  v0 = x0\n","  dv0dx0 = 1 # The derivative of a variable with respect to itself is 1\n","  v1 = x1\n","  dv1dx0 = 0 # v1 does not depend upon x0\n","  v2 = x2\n","  dv2dx0 = 0 # v2 does not depend upon x0\n","  v3 = np.sin(v0)\n","  dv3dx0 = dv0dx0 * np.cos(v0) # Chain rule: f(g((x)) f = sin(), g = v0, par f = cos(), par v0 = dv0dx0\n","  v4 = np.cos(v0)\n","  dv4dx0 = dv0dx0 * - np.sin(v0) # Chain rule, f(g((x)) f = cos(), g = v0, par f = -sin(), par v0 = dv0dx0\n","  v5 = v1 * v4\n","  dv5dx0 = v1 * dv4dx0 + v4 * dv1dx0 # Product rule, u = v1, v = v4, par u = dv1dx0, par v = dv4dx0\n","  v6 = v1/v2\n","  dv6dx0 = (v2*dv1dx0-v1*dv2dx0)/v2**2 # Quotient rule, u = v1, v = v2, par u = dv1dx0, par v = dv2dx0\n","  v7 = v3+v5\n","  dv7dx0 = dv3dx0 + dv5dx0 # Addition/subtraction of gradients is linear\n","  v8 = v6 + v7\n","  dv8dx0 = dv6dx0 + dv7dx0 # Addition/subtraction of gradients is linear\n","  y = v8\n","  dydx0 = dv8dx0\n","  return y, dydx0\n","```"],"metadata":{"id":"GpLAj-wwBJdk"}},{"cell_type":"markdown","source":["While forward mode automatic differentiation is relatively simple conceptually, and is useful for cases where the number of outputs is larger than the number of inputs, as separate calculations are required when we would like to calculate partial derivatives with respect to different inputs. This means it is well-suited to applications such as sensitivity analysis, where we would like to see how sensitive each output is to a particular input variable. At each intermediate variable, we calculate partial derivatives of the variable with respect to *input*. \\\\\n","However, in cases such as machine learning, where the number of parameters vastly outweighs the number of outputs, and we would like partial derivatives of the *output* with respect to parameters, a different approach must be taken. Here, it makes much more sense to propagate derivative information backwards through the graph. This way, the information stored on each of the intermediate variables is the partial derivative of the intermediate variable with respect to *output*, which is exactly what we need for our parameter updates. Propagating derivatives in this way is called reverse-mode automatic differentiation, and the backpropagation algorithm covered last week is itself a specific case of reverse-mode automatic differentiation."],"metadata":{"id":"ZQJErJt933O0"}},{"cell_type":"markdown","source":["# Reverse-Mode Automatic Differentiation\n","This process involves making a forward pass through the computational graph to calculate values on all of the intermediate variables, before making a backward pass through the graph and calculating the dependencies of intermediate variables with respect to the output of the function. Similarly to backpropagation, this involves calculating the partial derviative of intermediate variables with respect to everything ahead of it in the computational graph. To prevent expression swell, information for the total partial derivative of an intermediate variable with respect to all forward pathways is contained in a numerical value called the 'adjoint' of the intermediate variable, denoted by $\\bar{v}_i$, where: \\\n","$\\bar{v}_i = \\frac{\\partial v_i}{\\partial y} = \\sum\\limits_{j}\\bar{v}_j\\frac{\\partial v_j}{\\partial v_i}$ for all intermediate variables $v_j$ connected to $v_i$ by an outward arrow.\n","\n","Taking the functional form of $f(x_0,x_1) = (\\frac{x_0^2}{x_1})\\cdot \\mathrm{cos}(x_0) + e^{\\frac{x_0}{x_1}}$, and its computational graph:\n","\n","<img src='https://drive.google.com/uc?id=1VvOmzRom-v20HDUKyJJUWBlSm8vfdgHQ'>\n","\n","we can separate these two passes through the network:"],"metadata":{"id":"KyMacflj5a0F"}},{"cell_type":"code","source":["def function1_reverse(x0, x1):\n","  v0 = x0\n","  v1 = x1\n","  v2 = v0/v1\n","  v3 = v0 * v2\n","  v4 = np.exp(v2)\n","  v5 = np.cos(v0)\n","  v6 = v3 * v5\n","  v7 = v4 + v6\n","  y = v7\n","  # Backward Pass\n","  vbar7 = 1 # Equal to y, so partial derivative is 1\n","  # Considering v6: One pathway ahead, via v7\n","  dv7dv6 = 1 # v4 treated as constant, v6 differentiates to 1\n","  vbar6 = vbar7 * dv7dv6\n","  # Considering v5: One pathway ahead, via v6\n","  dv6dv5 = v3 # v3 treated as constant, derivative is v3 * dv6/dv6 (=1)\n","  vbar5 = vbar6 * dv6dv5\n","  # Considering v4: One pathway ahead, via v7\n","  dv7dv4 = 1 # v6 treated as constant, dv4/dv4=1\n","  vbar4 = vbar7 * dv7dv4\n","  # Considering v3: One pathway ahead, via v6\n","  dv6dv3 = v5 # v5 treated as constant, dv6/dv3=v5\n","  vbar3 = vbar6 * dv6dv3\n","  # Considering v2: Two pathways ahead, via v3 and v4\n","  dv3dv2 = v0 # v0 treated as constant, dv3/dv2=v0\n","  dv4dv2 = np.exp(v2) # d e^x/dx = e^x\n","  vbar2 = vbar3 * dv3dv2 + vbar4 * dv4dv2 # Sum the two terms\n","  # Considering v1: One pathway ahead, via v2\n","  dv2dv1 = -v0/v1**2 # v0 treated as constant, dv2/dv1=-v0/(v1^2)\n","  vbar1 = vbar2 * dv2dv1\n","  # Considering v0: Three pathways ahead, via v2, v3, and v5\n","  dv2dv0 = 1/v1 # v1 treated as constant, dv2/dv1=1/v1\n","  dv3dv0 = v2 # v2 treated as constant, dv3/dv1 = v2\n","  dv5dv0 = -np.sin(v0) # d cos(x)/dx=-sin(x)\n","  vbar0 = vbar2 * dv2dv0 + vbar3 * dv3dv0 + vbar5 * dv5dv0 # Sum the terms\n","  return y, vbar0, vbar1, vbar2, vbar3, vbar4, vbar5, vbar6, vbar7"],"metadata":{"id":"t4UN91d_YaTi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Whilst the procedure is a bit more complex, we can see for the above function, we are able to return the partial derivative of every intermediate variable with respect to the output. This is especially useful in the context of neural networks where the parameters of the neural network each feature as an intermediate variable on the computational graph, and hence we get the full derivative information with respect to a given output with one backward pass through the graph. Comparison between the evaluation time of symbolic methods and automatic methods now become more stark. Again, this is further compounded when there are more compounded operations, as is the case with deep neural networks."],"metadata":{"id":"4YOgtRETpWta"}},{"cell_type":"code","source":["def symbolic_f1_full(x0, x1):\n","  y = (x0**2)/x1 * np.cos(x0) + np.exp(x0/x1)\n","  vbar0 = (-np.sin(x0)*x0**2 + np.exp(x0/x1)+2*x0*np.cos(x0))/x1\n","  vbar1 = -(x0*(np.exp(x0/x1)+x0*np.cos(x0)))/x1**2\n","  vbar2 = np.exp(x0/x1) + x0*np.cos(x0)\n","  vbar3 = np.cos(x0)\n","  vbar4 = 1\n","  vbar5 = x0**2/x1\n","  vbar6 = 1\n","  vbar7 = 1\n","  return y, vbar0, vbar1, vbar2, vbar3, vbar4, vbar5, vbar6, vbar7\n","\n","\n","import timeit\n","time_automatic = timeit.timeit(lambda: function1_reverse(1,1), number=100000)\n","time_symbolic = timeit.timeit(lambda: symbolic_f1_full(1,1), number=100000)\n","print(f'Time taken by automatic differentiation: {time_automatic}')\n","print(f'Time taken by symbolic differentiation: {time_symbolic}')\n","\n","print('Automatic: '+str(function1_reverse(1.8,2.4)))\n","print('Symbolic: '+str(symbolic_f1_full(1.8,2.4)))"],"metadata":{"id":"KM0wwJX_3167","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746705915606,"user_tz":-60,"elapsed":2023,"user":{"displayName":"Ian Vidamour","userId":"11824567035710900920"}},"outputId":"527ce553-157c-4534-b00f-a52d37bd9edc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken by automatic differentiation: 0.6372098439987894\n","Time taken by symbolic differentiation: 1.3550646929998038\n","Automatic: (np.float64(1.810277188777007), np.float64(-0.7734141034699129), np.float64(-0.5337613269265994), np.float64(1.708036246165118), np.float64(-0.2272020946930871), 1, 1.35, 1, 1)\n","Symbolic: (np.float64(1.810277188777007), np.float64(-0.7734141034699131), np.float64(-0.5337613269265994), np.float64(1.708036246165118), np.float64(-0.2272020946930871), 1, 1.35, 1, 1)\n"]}]},{"cell_type":"markdown","source":["# Exercise:\n","Compose a the backward pass of a function to perform reverse-mode autodifferentiation on:\n","$y=\\mathrm{sin}(x_0) + x_1\\mathrm{cos}(x_0) + \\frac{x_1}{x_2}$\n","<img src='https://drive.google.com/uc?id=1XiDJ1Y_t7JO6h-k53wswnR-T5end9tS2'>"],"metadata":{"id":"rokPV_huyjJr"}},{"cell_type":"code","source":["def function(x0, x1, x2):\n","  # Forward Pass\n","  v0 = x0\n","  v1 = x1\n","  v2 = x2\n","  v3 = np.sin(v0)\n","  v4 = np.cos(v0)\n","  v5 = v1 * v4\n","  v6 = v1/v2\n","  v7 = v3+v5\n","  v8 = v6 + v7\n","\n","  # Fill in the backward pass\n","  return v8, vbar0, vbar1, vbar2, vbar3, vbar4, vbar5, vbar6, vbar7, vbar8"],"metadata":{"id":"JNKA8TS4uhkW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<details>\n","<summary>Show Solution</summary>\n","\n","```python\n","# Solution:\n","def function(x0, x1, x2):\n","  # Forward Pass\n","  v0 = x0\n","  v1 = x1\n","  v2 = x2\n","  v3 = np.sin(v0)\n","  v4 = np.cos(v0)\n","  v5 = v1 * v4\n","  v6 = v1/v2\n","  v7 = v3+v5\n","  v8 = v6 + v7\n","\n","  # Fill in the backward pass\n","  vbar8 = 1\n","  # Considering v7, one pathway via v8\n","  dv8dv7 = 1\n","  vbar7 = vbar8 * dv8dv7\n","  # Considering v6, one pathway via v8\n","  dv8dv6 = 1\n","  vbar6 = vbar8 * dv8dv6\n","  # Considering v5, one pathway via v7\n","  dv7dv5 = 1\n","  vbar5 = vbar7 * dv7dv5\n","  # Considering v4, one pathway via v5\n","  dv5dv4 = v1\n","  vbar4 = vbar5 * dv5dv4\n","  # Considering v3, one pathway via v7\n","  dv7dv3 = 1\n","  vbar3 = vbar7 * dv7dv3\n","  # Considering v2, one pathway via v6\n","  dv6dv2 = -1*v1/v2**2\n","  vbar2 = vbar6 * dv6dv2\n","  # Considering v1, two pathways via v5 and v6\n","  dv5dv1 = v4\n","  dv6dv1 = 1/v2\n","  vbar1 = vbar5 * dv5dv1 + vbar6 * dv6dv1\n","  # Considering v0, two pathways via v3 and v4\n","  dv3dv0 = cos(v0)\n","  dv4dv0 = -sin(v0)\n","  vbar0 = vbar3*dv3dv0 + vbar4*dv4dv0\n","  return v8, vbar0, vbar1, vbar2, vbar3, vbar4, vbar5, vbar6, vbar7, vbar8\n","```"],"metadata":{"id":"h17EzSl1B__e"}},{"cell_type":"markdown","source":["# Packages with automatic differentiation\n","While completing the exercises, you will have found that writing procedures for automatic differentiation by hand is quite cumbersome. Thankfully, modern ML packages such as PyTorch, TensorFlow, and Keras all come with tools that will procedurally perform automatic differentiation on functions/programs without the need to fill out all the terms by hand. In this section, we will do a brief introduction to using the autodiff functionality of PyTorch.\n","\n","First, we will create a class to make a small multi-layer perceptron (Note: PyTorch has tools that will help generate neural networks more simply, but this higher-level approach gives us greater flexibility):"],"metadata":{"id":"mKGBRfDBz0f4"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# A class for a simple multilayer perceptron. NetShape is a list of integers giving the shape\n","# of the network, act_fn should be a torch function which provides an activation function for nodes\n","# in the network, and lossfn is a loss function to use for optimisation (again a torch function)\n","class MLP(nn.Module):\n","    def __init__(self, NetShape, act_fn=nn.Tanh()):\n","        super(MLP, self).__init__()\n","        # Find number of layers in the network\n","        self.Nlayers = len(NetShape)-1\n","        # Initialise lists to store weights and biases\n","        self.weights = []\n","        self.biases = []\n","        # For each layer in the network, add parameters for weights and biases, and mark that gradients\n","        # should be tracked for these parameters\n","        for layer in range(self.Nlayers):\n","            self.weights.append(torch.rand((NetShape[layer], NetShape[layer+1]), requires_grad=True))\n","            self.biases.append(torch.zeros((NetShape[layer+1]), requires_grad=True))\n","        # Define an activation function for nodes in the network\n","        self.act_fn = act_fn\n","\n","    # Define a forward pass through our model\n","    def forward(self, x):\n","        # Pass through each layer of the model\n","        for layer in range(self.Nlayers):\n","            # Combine inputs/activations with weights, add biases, then perform activation function if not the output layer\n","            if layer < self.Nlayers-1:\n","                x = self.act_fn(torch.matmul(x, self.weights[layer])+self.biases[layer])\n","            else:\n","                x = torch.matmul(x, self.weights[layer])+self.biases[layer]\n","        return x"],"metadata":{"id":"yVTlXXg-z0FY","executionInfo":{"status":"ok","timestamp":1746709128465,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ian Vidamour","userId":"11824567035710900920"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["Next, we will define a toy function to train our neural network to replicate"],"metadata":{"id":"uWh4vWna9pqL"}},{"cell_type":"code","source":["# Define a simple function to fit\n","f = lambda x: torch.sqrt((x[:, [0]]/x[:, [1]])) * torch.sin(x[:, [0]]*x[:, [1]]) * torch.exp(x[:, [2]])\n","\n","# Generate a random input set\n","inputs = torch.rand((10000,3))\n","\n","# Generate targets from function\n","targets = f(inputs)\n"],"metadata":{"id":"U0aRMIyY9msk","executionInfo":{"status":"ok","timestamp":1746709129867,"user_tz":-60,"elapsed":178,"user":{"displayName":"Ian Vidamour","userId":"11824567035710900920"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["Now, we will initialise our model, training parameters, and a simple function to sample minibatches of data from input/output pairs:"],"metadata":{"id":"Eaj2JLlo9vJ7"}},{"cell_type":"code","source":["# Initialise a small model\n","model = MLP([3, 50, 50, 1], act_fn=nn.Sigmoid())\n","\n","# Generate an optimiser to help with our parameter updates\n","# Define learning rate\n","eta = 1e-2\n","# Create optimiser for the parameters within the model\n","optimiser = optim.Adam(params=model.weights+model.biases, lr=eta)\n","\n","# Define a function to sample minibatches of our training data\n","def gen_samples(Nbatch, inputs, outputs):\n","    # Create random indices to sample\n","    k = torch.randint(0, inputs.shape[0], (Nbatch,))\n","    return inputs[k], targets[k]\n","\n","# Define a loss function\n","lossfn = nn.MSELoss()\n"],"metadata":{"id":"tO0nlmR394yp","executionInfo":{"status":"ok","timestamp":1746709139074,"user_tz":-60,"elapsed":7087,"user":{"displayName":"Ian Vidamour","userId":"11824567035710900920"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["Finally, we will create a simple training procedure which will perform automatic differentiation, and update our parameters according to our loss function:"],"metadata":{"id":"b1Omabeq96wp"}},{"cell_type":"code","source":["# Create a simple training procedure\n","for i in range(10000):\n","    # Randomly sample inputs and targets for 100 training points\n","    x_in, y_in = gen_samples(100, inputs, targets)\n","    # Reset gradient calculation in our optimiser\n","    optimiser.zero_grad()\n","    # Pass forward through our model\n","    prediction = model.forward(x_in)\n","    # Calculate loss\n","    loss = lossfn(prediction, y_in)\n","    # Perform automatic differentiation for the parameters listed in the optimiser\n","    # with respect to loss\n","    loss.backward()\n","    # Use these gradients to step our parameters\n","    optimiser.step()\n","    # Print the loss of our neural network periodically\n","    if i%1000 == 0:\n","        print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zi0DEn7F-ENI","executionInfo":{"status":"ok","timestamp":1746709151270,"user_tz":-60,"elapsed":12197,"user":{"displayName":"Ian Vidamour","userId":"11824567035710900920"}},"outputId":"e3d07008-c74b-4c0e-b256-fdb1c33b5921"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["578.545166015625\n","0.0071814716793596745\n","0.001309192506596446\n","0.0014369856799021363\n","0.000721322896424681\n","0.00040197873022407293\n","0.00021226868557278067\n","0.00028325655148364604\n","0.0006178399198688567\n","0.0001817882584873587\n"]}]},{"cell_type":"markdown","source":["Within this training loop, we can access the partial derivatives of our parameters with respect to loss prior to stepping the optimiser:"],"metadata":{"id":"U3pdN5wS-Hcg"}},{"cell_type":"code","source":["# Randomly sample inputs and targets for 100 training points\n","x_in, y_in = gen_samples(100, inputs, targets)\n","# Reset gradient calculation in our optimiser\n","optimiser.zero_grad()\n","# Pass forward through our model\n","prediction = model.forward(x_in)\n","# Calculate loss\n","loss = lossfn(prediction, y_in)\n","# Perform automatic differentiation for the parameters listed in the optimiser\n","# with respect to loss\n","loss.backward()\n","# Considering the gradients for the first hidden layer weights:\n","layer1_weights = model.weights[0]\n","gradients = layer1_weights.grad\n","print(gradients)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8S_V3ipA-G0Y","executionInfo":{"status":"ok","timestamp":1746709171906,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ian Vidamour","userId":"11824567035710900920"}},"outputId":"b70aaf6e-422e-405b-8654-1ede819f6f69"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.7136e-04, -2.6757e-05, -6.8002e-06, -1.2225e-05, -2.0450e-05,\n","          2.8915e-05,  1.4314e-04, -8.6720e-05, -9.0563e-05, -8.3435e-05,\n","         -3.5260e-05,  1.7535e-04, -5.8361e-05,  1.3866e-04,  3.7111e-05,\n","         -6.2662e-05, -3.0924e-05,  1.6691e-04, -9.4369e-05, -1.0459e-04,\n","          5.7064e-05,  1.7134e-05, -2.7458e-05, -1.7193e-04,  1.4677e-05,\n","          9.6307e-05,  3.1639e-04, -7.0145e-05,  2.4450e-05,  3.8485e-05,\n","         -1.5795e-04, -6.4853e-06, -6.7634e-05,  1.7198e-04, -5.0767e-05,\n","          1.2763e-05, -8.1301e-05, -7.9354e-05, -2.9504e-05,  2.5092e-05,\n","         -1.7073e-04, -9.0513e-06, -2.6975e-05, -1.5169e-05, -4.7157e-05,\n","         -7.6108e-05,  3.7039e-05,  7.9694e-05,  1.5387e-04,  2.3383e-04],\n","        [ 3.9263e-04, -3.6777e-05,  5.9635e-05, -1.3911e-05,  6.7872e-06,\n","          1.0095e-04,  2.9050e-04, -1.3036e-04, -1.1155e-04, -2.1569e-04,\n","         -4.8599e-05,  2.4719e-04, -3.5942e-05,  2.5647e-04,  1.0344e-04,\n","         -1.6315e-04,  3.5849e-06,  2.4950e-04, -1.1047e-04, -2.1480e-04,\n","          1.0578e-04,  8.1550e-05,  1.5159e-05, -3.3450e-04,  6.5198e-05,\n","          2.2469e-04,  6.5012e-04, -1.1406e-04,  1.1546e-05,  6.4397e-05,\n","         -3.2838e-04,  4.0167e-05, -7.3166e-05,  3.4661e-04, -6.4976e-05,\n","          5.5939e-05, -1.4947e-04, -1.7731e-04, -6.0717e-05,  9.7663e-05,\n","         -3.6234e-04,  1.3323e-05, -5.2144e-06, -6.0300e-05, -2.1569e-05,\n","         -1.6156e-04,  3.2959e-05,  1.8622e-04,  3.4078e-04,  2.8571e-05],\n","        [ 2.7266e-04, -3.6633e-05, -1.5181e-06, -1.1789e-05, -2.5174e-05,\n","          4.0775e-05,  1.8372e-04, -1.0830e-04, -1.0456e-04, -1.1099e-04,\n","         -4.6818e-05,  1.3072e-04, -6.9715e-05,  1.8463e-04,  4.4691e-05,\n","         -8.8787e-05, -3.3315e-05,  1.6821e-04, -1.0585e-04, -1.3549e-04,\n","          5.8917e-05,  3.2162e-06, -2.6196e-05, -2.1571e-04,  1.0805e-05,\n","          1.4188e-04,  4.6072e-04, -8.4362e-05,  1.1032e-05,  3.3777e-05,\n","         -2.0760e-04, -1.5897e-06, -7.5936e-05,  2.5628e-04, -5.8630e-05,\n","         -1.7312e-05, -1.0237e-04, -9.9080e-05, -2.8753e-05,  3.6253e-05,\n","         -2.1234e-04, -1.0529e-05, -3.1133e-05, -2.6412e-05, -5.1353e-05,\n","         -9.7595e-05,  3.9518e-05,  1.1567e-04,  2.2006e-04,  1.3468e-04]])\n"]}]},{"cell_type":"markdown","source":["Here, we can see how simple it is to use the in-built functions within PyTorch to build a neural network, and perform parameter updates according to gradients. The entire process of building computational graph, and propagating derivatives backwards through the graph is handled internally by PyTorch. If we would like to see those gradients, we can simply access them by calling the grad attribute of the tensor class."],"metadata":{"id":"0gdt55pE-x-L"}}]}